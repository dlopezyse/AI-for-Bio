## Large Language Models in Biology ðŸ¤–

| Name | Description |
| :--- | :--- | 
| [BioBERT](https://github.com/dmis-lab/biobert) | BioBERT, a biomedical language representation model designed for biomedical text mining tasks such as biomedical named entity recognition, relation extraction, question answering, etc. Please refer to the paper [BioBERT: a pre-trained biomedical language representation model for biomedical text mining](https://academic.oup.com/bioinformatics/article/36/4/1234/5566506) for more details.
| [BioGPT](https://github.com/microsoft/BioGPT) | The BioGPT model was proposed in [BioGPT: generative pre-trained transformer for biomedical text generation and mining](https://academic.oup.com/bib/article/23/6/bbac409/6713511?guestAccessKey=a66d9b5d-4f83-4017-bb52-405815c907b9&login=false) by Renqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon and Tie-Yan Liu. BioGPT is a domain-specific generative pre-trained Transformer language model for biomedical text generation and mining. BioGPT follows the Transformer language model backbone, and is pre-trained on 15M PubMed abstracts from scratch.
| [BioinspiredLLM](https://huggingface.co/lamm-mit/BioinspiredLLM) | [BioinspiredLLM](https://arxiv.org/abs/2309.08788) was finetuned with a corpus of over a thousand peer-reviewed articles in the field of structural biological and bio-inspired materials and can be prompted to recall information, assist with research tasks, and function as an engine for creativity. The model has proven that it is able to accurately recall information about biological materials and is further enhanced with enhanced reasoning ability, as well as with retrieval-augmented generation to incorporate new data during generation that can also help to traceback sources, update the knowledge base, and connect knowledge domains.
| [BioMedGPT](https://github.com/PharMolix/OpenBioMed?tab=readme-ov-file) | [BioMedGPT](https://arxiv.org/abs/2308.09442) is the first commercial-friendly multimodal biomedical foundation model jointly released by PharMolix and the Institute of AI Industry Research (AIR). It aligns the language of life (molecular structures and protein sequences) with human natural language, performing on par with human experts on biomedical QA benchmarks and demonstrating powerful performance in cross-modal molecule and protein question-answering tasks.
| [Chemformer](https://github.com/MolecularAI/Chemformer) | The [Chemformer](https://iopscience.iop.org/article/10.1088/2632-2153/ac3ffb/meta) project aimed to pre-train a BART transformer language model [3] on molecular SMILES strings [4] by optimising a de-noising objective. We hypothesized that pre-training would lead to improved generalisation, performance, training speed and validity on downstream fine-tuned tasks. The pre-trained model was tested on downstream tasks such as reaction prediction, retrosynthetic prediction, molecular optimisation and molecular property prediction
| [ChemLLM](https://huggingface.co/AI4Chem/ChemLLM-7B-Chat) | [ChemLLM](https://arxiv.org/abs/2402.06852) is a comprehensive framework that features the first LLM dedicated to chemistry. It also includes ChemData, a dataset specifically designed for instruction tuning, and ChemBench, a robust benchmark covering nine essential chemistry tasks. ChemLLM is adept at performing various tasks across chemical disciplines with fluid dialogue interaction.
| [LlaSMol](https://osu-nlp-group.github.io/LLM4Chem/) | [LlaSMol](https://arxiv.org/abs/2402.09391) (large language models for small molecules) is a series of LLMs built for conducting various chemistry tasks. Specifically, we use Galactica, Llama 2, Code Llama, and Mistral as the base models, and conduct instruction tuning with LoRA on our SMolInstruct dataset.
| [Mol-BERT](https://github.com/cxfjiang/MolBERT) | We present a novel end-to-end deep learning framework, named [Mol-BERT](https://www.hindawi.com/journals/wcmc/2021/7181815/), that combines an effective molecular representation with pretrained BERT model tailored for molecular property prediction. Specifically, a large-scale prediction BERT model is pretrained to generate the embedding of molecular substructures, by using four million unlabeled drug SMILES (i.e., ZINC 15 and ChEMBL 27). Then, the pretrained BERT model can be fine-tuned on various molecular property prediction tasks.
| [Mol-GPT](https://github.com/devalab/molgpt) | The representation of molecules in SMILES notation as a string of characters enables the usage of state of the art models in natural language processing, such as Transformers, for molecular design in general. Inspired by generative pre-training (GPT) models that have been shown to be successful in generating meaningful text, we train a transformer-decoder on the next token prediction task using masked self-attention for the generation of druglike molecules in this study. We show that our model, [MolGPT](https://pubs.acs.org/doi/abs/10.1021/acs.jcim.1c00600), performs on par with other previously proposed modern machine learning frameworks for molecular generation in terms of generating valid, unique, and novel molecules. Furthermore, we demonstrate that the model can be trained conditionally to control multiple properties of the generated molecules. 
| [ProteinBERT](https://github.com/nadavbra/protein_bert) | [ProteinBERT](https://academic.oup.com/bioinformatics/article/38/8/2102/6502274?login=false) is a protein language model pretrained on ~106M proteins from UniRef90. The pretrained model can be fine-tuned on any protein-related task in a matter of minutes. ProteinBERT achieves state-of-the-art performance on a wide range of benchmarks. ProteinBERT is built on Keras/TensorFlow. ProteinBERT's deep-learning architecture is inspired by BERT, but contains several innovations such as global-attention layers that have linear complexity for sequence length (compared to self-attention's quadratic/n^2 growth). As a result, the model can process protein sequences of almost any length, including extremely long protein sequences (of over tens of thousands of amino acids). The model takes protein sequences as inputs, and can also take protein GO annotations as additional inputs (to help the model infer about the function of the input protein and update its internal representations and outputs accordingly).
| [SELFormer](https://github.com/HUBioDataLab/SELFormer) | One approach to efficiently learning molecular representations is processing string-based notations of chemicals via natural language processing (NLP) algorithms. Most of the methods proposed so far utilize SMILES notations for this purpose; however, SMILES is associated with numerous problems related to validity and robustness, which may prevent the model from effectively uncovering the knowledge hidden in the data. In this study, we propose [SELFormer](https://arxiv.org/abs/2304.04662), a transformer architecture-based chemical language model that utilizes a 100% valid, compact, and expressive notation, SELFIES, as input in order to learn flexible and high-quality molecular representations. SELFormer is pre-trained on two million drug-like compounds and fine-tuned for diverse molecular property prediction tasks. 
| [Uni-Mol](https://github.com/dptech-corp/Uni-Mol) | [Uni-Mol](https://chemrxiv.org/engage/chemrxiv/article-details/6402990d37e01856dc1d1581) is a universal 3D molecular pretraining framework that significantly expands representation capacity and application scope in drug design. The framework comprises two models: a molecular pretraining model that has been trained using 209M molecular 3D conformations, and a pocket pretraining model that has been trained using 3M candidate protein pocket data. These two models can be used independently for different tasks and are combined for protein-ligand binding tasks. Uni-Mol has demonstrated superior performance compared to the state-of-the-art (SOTA) in 14 out of 15 molecular property prediction tasks. Moreover, Uni-Mol has achieved exceptional accuracy in 3D spatial tasks, such as protein-ligand binding pose prediction and molecular conformation generation.
